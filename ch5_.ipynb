{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple LSTM\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "import torch\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/sentiment_labelled_sentences/sentiment.txt\") as f:\n",
    "    reviews = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =  pd.DataFrame([review.split(\"\\t\") for review in reviews.split(\"\\n\")])\n",
    "data.columns = [\"review\", 'sentiment']\n",
    "data = data.sample(frac=1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>One more thing: I can tolerate political incor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>That's right....the red velvet cake.....ohhh t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1587</th>\n",
       "      <td>I hate those things as much as cheap quality b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2941</th>\n",
       "      <td>I have tried these cables with my computer and...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2980</th>\n",
       "      <td>Echo Problem....Very unsatisfactory</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review sentiment\n",
       "311   One more thing: I can tolerate political incor...         0\n",
       "1025  That's right....the red velvet cake.....ohhh t...         1\n",
       "1587  I hate those things as much as cheap quality b...         0\n",
       "2941  I have tried these cables with my computer and...         1\n",
       "2980                Echo Problem....Very unsatisfactory         0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_words_reviews(data):\n",
    "    text = data.review.tolist()\n",
    "    clean_text = []\n",
    "    for t in text:\n",
    "        clean_text.append(t.translate(str.maketrans('', '', punctuation)).lower().rstrip())\n",
    "    tokenized = [word_tokenize(x) for x in clean_text]\n",
    "    all_text  = []\n",
    "    for tokens in tokenized:\n",
    "        for to in tokens:\n",
    "            all_text.append(to)\n",
    "    return tokenized, set(all_text)\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['one',\n",
       " 'more',\n",
       " 'thing',\n",
       " 'i',\n",
       " 'can',\n",
       " 'tolerate',\n",
       " 'political',\n",
       " 'incorrectness',\n",
       " 'very',\n",
       " 'well',\n",
       " 'im',\n",
       " 'all',\n",
       " 'for',\n",
       " 'artistic',\n",
       " 'freedom',\n",
       " 'and',\n",
       " 'suspension',\n",
       " 'of',\n",
       " 'disbelief',\n",
       " 'but',\n",
       " 'the',\n",
       " 'slavic',\n",
       " 'female',\n",
       " 'character',\n",
       " 'was',\n",
       " 'just',\n",
       " 'too',\n",
       " 'much']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews, vocab = split_words_reviews(data)\n",
    "labels = np.array([int(x) for x in data['sentiment'].values])\n",
    "reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assemble lookuo dictionaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionaries(words):\n",
    "\n",
    "    word_to_int_dict = {w: i+1 for i, w in enumerate(words)}\n",
    "    int_to_word_dict = {i: w for w, i in word_to_int_dict.items()}\n",
    "    return word_to_int_dict, int_to_word_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: '010',\n",
       " 2: '1',\n",
       " 3: '10',\n",
       " 4: '100',\n",
       " 5: '1010',\n",
       " 6: '11',\n",
       " 7: '110',\n",
       " 8: '1199',\n",
       " 9: '12',\n",
       " 10: '13',\n",
       " 11: '15',\n",
       " 12: '15lb',\n",
       " 13: '17',\n",
       " 14: '18',\n",
       " 15: '18th',\n",
       " 16: '1928',\n",
       " 17: '1947',\n",
       " 18: '1948',\n",
       " 19: '1949',\n",
       " 20: '1971',\n",
       " 21: '1973',\n",
       " 22: '1979',\n",
       " 23: '1980s',\n",
       " 24: '1986',\n",
       " 25: '1995',\n",
       " 26: '1998',\n",
       " 27: '2',\n",
       " 28: '20',\n",
       " 29: '2000',\n",
       " 30: '2005',\n",
       " 31: '2006',\n",
       " 32: '2007',\n",
       " 33: '20th',\n",
       " 34: '20the',\n",
       " 35: '2160',\n",
       " 36: '23',\n",
       " 37: '24',\n",
       " 38: '25',\n",
       " 39: '2mp',\n",
       " 40: '3',\n",
       " 41: '30',\n",
       " 42: '30s',\n",
       " 43: '325',\n",
       " 44: '34ths',\n",
       " 45: '35',\n",
       " 46: '350',\n",
       " 47: '375',\n",
       " 48: '3o',\n",
       " 49: '4',\n",
       " 50: '40',\n",
       " 51: '400',\n",
       " 52: '40min',\n",
       " 53: '42',\n",
       " 54: '45',\n",
       " 55: '4s',\n",
       " 56: '5',\n",
       " 57: '50',\n",
       " 58: '5020',\n",
       " 59: '510',\n",
       " 60: '5320',\n",
       " 61: '54',\n",
       " 62: '5of',\n",
       " 63: '5year',\n",
       " 64: '6',\n",
       " 65: '680',\n",
       " 66: '7',\n",
       " 67: '70',\n",
       " 68: '70000',\n",
       " 69: '700w',\n",
       " 70: '70s',\n",
       " 71: '744',\n",
       " 72: '750',\n",
       " 73: '785',\n",
       " 74: '8',\n",
       " 75: '80',\n",
       " 76: '80s',\n",
       " 77: '810',\n",
       " 78: '8125',\n",
       " 79: '815pm',\n",
       " 80: '8525',\n",
       " 81: '8530',\n",
       " 82: '8pm',\n",
       " 83: '9',\n",
       " 84: '90',\n",
       " 85: '90s',\n",
       " 86: '910',\n",
       " 87: '95',\n",
       " 88: 'a',\n",
       " 89: 'aailiyah',\n",
       " 90: 'abandoned',\n",
       " 91: 'abhor',\n",
       " 92: 'ability',\n",
       " 93: 'able',\n",
       " 94: 'abound',\n",
       " 95: 'about',\n",
       " 96: 'above',\n",
       " 97: 'abovepretty',\n",
       " 98: 'abroad',\n",
       " 99: 'absolute',\n",
       " 100: 'absolutel',\n",
       " 101: 'absolutely',\n",
       " 102: 'absolutley',\n",
       " 103: 'abstruse',\n",
       " 104: 'abysmal',\n",
       " 105: 'ac',\n",
       " 106: 'academy',\n",
       " 107: 'accents',\n",
       " 108: 'accept',\n",
       " 109: 'acceptable',\n",
       " 110: 'access',\n",
       " 111: 'accessable',\n",
       " 112: 'accessible',\n",
       " 113: 'accessing',\n",
       " 114: 'accessory',\n",
       " 115: 'accessoryone',\n",
       " 116: 'accident',\n",
       " 117: 'accidentally',\n",
       " 118: 'acclaimed',\n",
       " 119: 'accolades',\n",
       " 120: 'accommodations',\n",
       " 121: 'accomodate',\n",
       " 122: 'accompanied',\n",
       " 123: 'according',\n",
       " 124: 'accordingly',\n",
       " 125: 'accountant',\n",
       " 126: 'accurate',\n",
       " 127: 'accurately',\n",
       " 128: 'accused',\n",
       " 129: 'ache',\n",
       " 130: 'achievement',\n",
       " 131: 'achille',\n",
       " 132: 'ackerman',\n",
       " 133: 'acknowledged',\n",
       " 134: 'across',\n",
       " 135: 'act',\n",
       " 136: 'acted',\n",
       " 137: 'acting',\n",
       " 138: 'actingeven',\n",
       " 139: 'actingwise',\n",
       " 140: 'action',\n",
       " 141: 'actions',\n",
       " 142: 'activate',\n",
       " 143: 'activated',\n",
       " 144: 'activesync',\n",
       " 145: 'actor',\n",
       " 146: 'actors',\n",
       " 147: 'actorsan',\n",
       " 148: 'actress',\n",
       " 149: 'actresses',\n",
       " 150: 'actual',\n",
       " 151: 'actually',\n",
       " 152: 'ad',\n",
       " 153: 'adams',\n",
       " 154: 'adaptation',\n",
       " 155: 'adapter',\n",
       " 156: 'adapters',\n",
       " 157: 'add',\n",
       " 158: 'added',\n",
       " 159: 'addition',\n",
       " 160: 'additional',\n",
       " 161: 'address',\n",
       " 162: 'adhesive',\n",
       " 163: 'admins',\n",
       " 164: 'admiration',\n",
       " 165: 'admitted',\n",
       " 166: 'adorable',\n",
       " 167: 'adorablehis',\n",
       " 168: 'adorablethe',\n",
       " 169: 'adrift',\n",
       " 170: 'adventure',\n",
       " 171: 'advertised',\n",
       " 172: 'advise',\n",
       " 173: 'aerial',\n",
       " 174: 'aesthetically',\n",
       " 175: 'affected',\n",
       " 176: 'affleck',\n",
       " 177: 'affordable',\n",
       " 178: 'afraid',\n",
       " 179: 'africa',\n",
       " 180: 'after',\n",
       " 181: 'afternoon',\n",
       " 182: 'again',\n",
       " 183: 'against',\n",
       " 184: 'age',\n",
       " 185: 'ages',\n",
       " 186: 'aggravating',\n",
       " 187: 'ago',\n",
       " 188: 'agree',\n",
       " 189: 'agreed',\n",
       " 190: 'ahead',\n",
       " 191: 'aimless',\n",
       " 192: 'air',\n",
       " 193: 'aired',\n",
       " 194: 'airline',\n",
       " 195: 'airport',\n",
       " 196: 'akasha',\n",
       " 197: 'akin',\n",
       " 198: 'ala',\n",
       " 199: 'alarm',\n",
       " 200: 'albondigas',\n",
       " 201: 'alert',\n",
       " 202: 'alexander',\n",
       " 203: 'alike',\n",
       " 204: 'all',\n",
       " 205: 'allergy',\n",
       " 206: 'allison',\n",
       " 207: 'allot',\n",
       " 208: 'allow',\n",
       " 209: 'allowing',\n",
       " 210: 'allows',\n",
       " 211: 'allstar',\n",
       " 212: 'almonds',\n",
       " 213: 'almost',\n",
       " 214: 'alone',\n",
       " 215: 'along',\n",
       " 216: 'alongside',\n",
       " 217: 'alot',\n",
       " 218: 'already',\n",
       " 219: 'also',\n",
       " 220: 'although',\n",
       " 221: 'aluminum',\n",
       " 222: 'always',\n",
       " 223: 'am',\n",
       " 224: 'amateurish',\n",
       " 225: 'amaze',\n",
       " 226: 'amazed',\n",
       " 227: 'amazing',\n",
       " 228: 'amazingly',\n",
       " 229: 'amazingrge',\n",
       " 230: 'amazingstylized',\n",
       " 231: 'amazon',\n",
       " 232: 'ambiance',\n",
       " 233: 'ambience',\n",
       " 234: 'america',\n",
       " 235: 'american',\n",
       " 236: 'americans',\n",
       " 237: 'americas',\n",
       " 238: 'among',\n",
       " 239: 'amount',\n",
       " 240: 'amp',\n",
       " 241: 'ample',\n",
       " 242: 'amusing',\n",
       " 243: 'an',\n",
       " 244: 'anatomist',\n",
       " 245: 'and',\n",
       " 246: 'andddd',\n",
       " 247: 'andor',\n",
       " 248: 'angel',\n",
       " 249: 'angela',\n",
       " 250: 'angeles',\n",
       " 251: 'angelina',\n",
       " 252: 'angle',\n",
       " 253: 'angles',\n",
       " 254: 'angry',\n",
       " 255: 'anguish',\n",
       " 256: 'angus',\n",
       " 257: 'animals',\n",
       " 258: 'animated',\n",
       " 259: 'animation',\n",
       " 260: 'anita',\n",
       " 261: 'ann',\n",
       " 262: 'anne',\n",
       " 263: 'annes',\n",
       " 264: 'anniversary',\n",
       " 265: 'annoying',\n",
       " 266: 'another',\n",
       " 267: 'answer',\n",
       " 268: 'ant',\n",
       " 269: 'antena',\n",
       " 270: 'anthony',\n",
       " 271: 'anticipated',\n",
       " 272: 'antiglare',\n",
       " 273: 'antithesis',\n",
       " 274: 'any',\n",
       " 275: 'anymore',\n",
       " 276: 'anyone',\n",
       " 277: 'anything',\n",
       " 278: 'anythinga',\n",
       " 279: 'anytime',\n",
       " 280: 'anyway',\n",
       " 281: 'anyways',\n",
       " 282: 'anywhere',\n",
       " 283: 'apart',\n",
       " 284: 'apartment',\n",
       " 285: 'apologize',\n",
       " 286: 'apology',\n",
       " 287: 'app',\n",
       " 288: 'appalling',\n",
       " 289: 'apparently',\n",
       " 290: 'appealing',\n",
       " 291: 'appearance',\n",
       " 292: 'appears',\n",
       " 293: 'appetite',\n",
       " 294: 'appetizer',\n",
       " 295: 'appetizers',\n",
       " 296: 'applauded',\n",
       " 297: 'applause',\n",
       " 298: 'apple',\n",
       " 299: 'applifies',\n",
       " 300: 'appointments',\n",
       " 301: 'appreciate',\n",
       " 302: 'appropriate',\n",
       " 303: 'approval',\n",
       " 304: 'apt',\n",
       " 305: 'are',\n",
       " 306: 'area',\n",
       " 307: 'arent',\n",
       " 308: 'arepas',\n",
       " 309: 'argued',\n",
       " 310: 'arguing',\n",
       " 311: 'aria',\n",
       " 312: 'armageddon',\n",
       " 313: 'armand',\n",
       " 314: 'armband',\n",
       " 315: 'around',\n",
       " 316: 'array',\n",
       " 317: 'arrival',\n",
       " 318: 'arrived',\n",
       " 319: 'arrives',\n",
       " 320: 'arriving',\n",
       " 321: 'art',\n",
       " 322: 'article',\n",
       " 323: 'articulated',\n",
       " 324: 'artiness',\n",
       " 325: 'artist',\n",
       " 326: 'artistic',\n",
       " 327: 'artless',\n",
       " 328: 'arts',\n",
       " 329: 'as',\n",
       " 330: 'asia',\n",
       " 331: 'aside',\n",
       " 332: 'ask',\n",
       " 333: 'asked',\n",
       " 334: 'asking',\n",
       " 335: 'asleep',\n",
       " 336: 'aspect',\n",
       " 337: 'aspects',\n",
       " 338: 'ass',\n",
       " 339: 'assante',\n",
       " 340: 'assaulted',\n",
       " 341: 'assistant',\n",
       " 342: 'assumed',\n",
       " 343: 'assure',\n",
       " 344: 'astonishingly',\n",
       " 345: 'astronaut',\n",
       " 346: 'astronauts',\n",
       " 347: 'at',\n",
       " 348: 'ate',\n",
       " 349: 'atleast',\n",
       " 350: 'atmosphere',\n",
       " 351: 'atmosphere1',\n",
       " 352: 'atrocious',\n",
       " 353: 'atrocity',\n",
       " 354: 'att',\n",
       " 355: 'attached',\n",
       " 356: 'attack',\n",
       " 357: 'attacked',\n",
       " 358: 'attempt',\n",
       " 359: 'attempted',\n",
       " 360: 'attempting',\n",
       " 361: 'attempts',\n",
       " 362: 'attention',\n",
       " 363: 'attentive',\n",
       " 364: 'attitudes',\n",
       " 365: 'attractive',\n",
       " 366: 'audience',\n",
       " 367: 'audio',\n",
       " 368: 'auju',\n",
       " 369: 'aurvåg',\n",
       " 370: 'austens',\n",
       " 371: 'austere',\n",
       " 372: 'authentic',\n",
       " 373: 'author',\n",
       " 374: 'auto',\n",
       " 375: 'autoanswer',\n",
       " 376: 'available',\n",
       " 377: 'average',\n",
       " 378: 'aversion',\n",
       " 379: 'avocado',\n",
       " 380: 'avoid',\n",
       " 381: 'avoided',\n",
       " 382: 'avoiding',\n",
       " 383: 'award',\n",
       " 384: 'awarded',\n",
       " 385: 'awards',\n",
       " 386: 'away',\n",
       " 387: 'awesome',\n",
       " 388: 'awful',\n",
       " 389: 'awkward',\n",
       " 390: 'awkwardly',\n",
       " 391: 'awsome',\n",
       " 392: 'ayce',\n",
       " 393: 'aye',\n",
       " 394: 'az',\n",
       " 395: 'b',\n",
       " 396: 'baaaaaad',\n",
       " 397: 'baba',\n",
       " 398: 'babbling',\n",
       " 399: 'babie',\n",
       " 400: 'baby',\n",
       " 401: 'babysitting',\n",
       " 402: 'bachi',\n",
       " 403: 'back',\n",
       " 404: 'backdrop',\n",
       " 405: 'backed',\n",
       " 406: 'background',\n",
       " 407: 'backlight',\n",
       " 408: 'bacon',\n",
       " 409: 'bad',\n",
       " 410: 'badass',\n",
       " 411: 'badly',\n",
       " 412: 'badwellits',\n",
       " 413: 'bagels',\n",
       " 414: 'baileys',\n",
       " 415: 'bakery',\n",
       " 416: 'baklava',\n",
       " 417: 'balance',\n",
       " 418: 'balanced',\n",
       " 419: 'ball',\n",
       " 420: 'ballet',\n",
       " 421: 'balls',\n",
       " 422: 'bamboo',\n",
       " 423: 'banana',\n",
       " 424: 'band',\n",
       " 425: 'bank',\n",
       " 426: 'bar',\n",
       " 427: 'barcelona',\n",
       " 428: 'bare',\n",
       " 429: 'barely',\n",
       " 430: 'bargain',\n",
       " 431: 'barking',\n",
       " 432: 'barney',\n",
       " 433: 'barren',\n",
       " 434: 'bars',\n",
       " 435: 'bartender',\n",
       " 436: 'bartenders',\n",
       " 437: 'baseball',\n",
       " 438: 'based',\n",
       " 439: 'basement',\n",
       " 440: 'basic',\n",
       " 441: 'basically',\n",
       " 442: 'bat',\n",
       " 443: 'batch',\n",
       " 444: 'bates',\n",
       " 445: 'bathroom',\n",
       " 446: 'bathrooms',\n",
       " 447: 'batter',\n",
       " 448: 'batteries',\n",
       " 449: 'battery',\n",
       " 450: 'baxendale',\n",
       " 451: 'bay',\n",
       " 452: 'bbq',\n",
       " 453: 'be',\n",
       " 454: 'be3',\n",
       " 455: 'beall',\n",
       " 456: 'bean',\n",
       " 457: 'beans',\n",
       " 458: 'bear',\n",
       " 459: 'beat',\n",
       " 460: 'beateous',\n",
       " 461: 'beats',\n",
       " 462: 'beautiful',\n",
       " 463: 'beautifully',\n",
       " 464: 'beauty',\n",
       " 465: 'became',\n",
       " 466: 'because',\n",
       " 467: 'bechard',\n",
       " 468: 'become',\n",
       " 469: 'becomes',\n",
       " 470: 'bed',\n",
       " 471: 'beef',\n",
       " 472: 'been',\n",
       " 473: 'beensteppedinandtrackedeverywhere',\n",
       " 474: 'beep',\n",
       " 475: 'beeping',\n",
       " 476: 'beer',\n",
       " 477: 'beers',\n",
       " 478: 'before',\n",
       " 479: 'beforei',\n",
       " 480: 'began',\n",
       " 481: 'begin',\n",
       " 482: 'beginning',\n",
       " 483: 'behind',\n",
       " 484: 'behing',\n",
       " 485: 'behold',\n",
       " 486: 'being',\n",
       " 487: 'bela',\n",
       " 488: 'believable',\n",
       " 489: 'believe',\n",
       " 490: 'believed',\n",
       " 491: 'bell',\n",
       " 492: 'bellagio',\n",
       " 493: 'bellies',\n",
       " 494: 'bells',\n",
       " 495: 'bellucci',\n",
       " 496: 'belly',\n",
       " 497: 'belmondo',\n",
       " 498: 'below',\n",
       " 499: 'belowpar',\n",
       " 500: 'belt',\n",
       " 501: 'ben',\n",
       " 502: 'bend',\n",
       " 503: 'bennett',\n",
       " 504: 'bergens',\n",
       " 505: 'bertolucci',\n",
       " 506: 'besides',\n",
       " 507: 'best',\n",
       " 508: 'bethe',\n",
       " 509: 'better',\n",
       " 510: 'betty',\n",
       " 511: 'between',\n",
       " 512: 'beware',\n",
       " 513: 'beyond',\n",
       " 514: 'bible',\n",
       " 515: 'big',\n",
       " 516: 'bigbudget',\n",
       " 517: 'bigger',\n",
       " 518: 'biggest',\n",
       " 519: 'bill',\n",
       " 520: 'bills',\n",
       " 521: 'billy',\n",
       " 522: 'binge',\n",
       " 523: 'biographical',\n",
       " 524: 'bipolarity',\n",
       " 525: 'bird',\n",
       " 526: 'biscuit',\n",
       " 527: 'biscuits',\n",
       " 528: 'bisque',\n",
       " 529: 'bit',\n",
       " 530: 'bitches',\n",
       " 531: 'bitchy',\n",
       " 532: 'bite',\n",
       " 533: 'bites',\n",
       " 534: 'bitpim',\n",
       " 535: 'bits',\n",
       " 536: 'black',\n",
       " 537: 'blackberry',\n",
       " 538: 'blacks',\n",
       " 539: 'blacktop',\n",
       " 540: 'blah',\n",
       " 541: 'blake',\n",
       " 542: 'blame',\n",
       " 543: 'bland',\n",
       " 544: 'blandest',\n",
       " 545: 'blandly',\n",
       " 546: 'blanket',\n",
       " 547: 'blare',\n",
       " 548: 'blatant',\n",
       " 549: 'blew',\n",
       " 550: 'blist',\n",
       " 551: 'block',\n",
       " 552: 'bloddy',\n",
       " 553: 'blood',\n",
       " 554: 'bloodiest',\n",
       " 555: 'bloody',\n",
       " 556: 'blow',\n",
       " 557: 'blown',\n",
       " 558: 'blows',\n",
       " 559: 'blue',\n",
       " 560: 'blueant',\n",
       " 561: 'bluegreenscreen',\n",
       " 562: 'bluetoooth',\n",
       " 563: 'bluetooth',\n",
       " 564: 'bluetoothmotorola',\n",
       " 565: 'bluetooths',\n",
       " 566: 'blush',\n",
       " 567: 'bmw',\n",
       " 568: 'boasts',\n",
       " 569: 'bob',\n",
       " 570: 'boba',\n",
       " 571: 'bodes',\n",
       " 572: 'body',\n",
       " 573: 'bohemian',\n",
       " 574: 'boiled',\n",
       " 575: 'boiling',\n",
       " 576: 'bold',\n",
       " 577: 'bombardments',\n",
       " 578: 'bond',\n",
       " 579: 'bone',\n",
       " 580: 'bonus',\n",
       " 581: 'bonuses',\n",
       " 582: 'boobs',\n",
       " 583: 'boogeyman',\n",
       " 584: 'book',\n",
       " 585: 'booking',\n",
       " 586: 'booksomethats',\n",
       " 587: 'boost',\n",
       " 588: 'boot',\n",
       " 589: 'bop',\n",
       " 590: 'bordered',\n",
       " 591: 'borderlines',\n",
       " 592: 'borders',\n",
       " 593: 'bore',\n",
       " 594: 'bored',\n",
       " 595: 'boring',\n",
       " 596: 'boringpointless',\n",
       " 597: 'borrowed',\n",
       " 598: 'bose',\n",
       " 599: 'boss',\n",
       " 600: 'both',\n",
       " 601: 'bother',\n",
       " 602: 'bothersome',\n",
       " 603: 'bottom',\n",
       " 604: 'bottowm',\n",
       " 605: 'bouchon',\n",
       " 606: 'bought',\n",
       " 607: 'bougth',\n",
       " 608: 'bowl',\n",
       " 609: 'box',\n",
       " 610: 'boxes',\n",
       " 611: 'boy',\n",
       " 612: 'boyfriend',\n",
       " 613: 'boyle',\n",
       " 614: 'boys',\n",
       " 615: 'brain',\n",
       " 616: 'brainsucking',\n",
       " 617: 'brand',\n",
       " 618: 'brat',\n",
       " 619: 'bread',\n",
       " 620: 'break',\n",
       " 621: 'breakage',\n",
       " 622: 'breakfast',\n",
       " 623: 'breakfastlunch',\n",
       " 624: 'breaking',\n",
       " 625: 'breaks',\n",
       " 626: 'breeders',\n",
       " 627: 'breeze',\n",
       " 628: 'brevity',\n",
       " 629: 'brian',\n",
       " 630: 'brick',\n",
       " 631: 'brief',\n",
       " 632: 'brigand',\n",
       " 633: 'bright',\n",
       " 634: 'brilliance',\n",
       " 635: 'brilliant',\n",
       " 636: 'brilliantly',\n",
       " 637: 'bring',\n",
       " 638: 'brings',\n",
       " 639: 'brisket',\n",
       " 640: 'broad',\n",
       " 641: 'broke',\n",
       " 642: 'broken',\n",
       " 643: 'brokeni',\n",
       " 644: 'brooding',\n",
       " 645: 'brother',\n",
       " 646: 'brought',\n",
       " 647: 'brownish',\n",
       " 648: 'browser',\n",
       " 649: 'browsing',\n",
       " 650: 'brunch',\n",
       " 651: 'bruschetta',\n",
       " 652: 'brushfire',\n",
       " 653: 'brutal',\n",
       " 654: 'bt',\n",
       " 655: 'bt250v',\n",
       " 656: 'bt50',\n",
       " 657: 'bubbling',\n",
       " 658: 'bucks',\n",
       " 659: 'buddy',\n",
       " 660: 'budget',\n",
       " 661: 'buds',\n",
       " 662: 'buffalo',\n",
       " 663: 'buffet',\n",
       " 664: 'buffets',\n",
       " 665: 'bug',\n",
       " 666: 'build',\n",
       " 667: 'builders',\n",
       " 668: 'building',\n",
       " 669: 'buildings',\n",
       " 670: 'buildup',\n",
       " 671: 'built',\n",
       " 672: 'buldogis',\n",
       " 673: 'bulky',\n",
       " 674: 'bullock',\n",
       " 675: 'bully',\n",
       " 676: 'bumpers',\n",
       " 677: 'bunch',\n",
       " 678: 'burger',\n",
       " 679: 'burgers',\n",
       " 680: 'burned',\n",
       " 681: 'burrittos',\n",
       " 682: 'burtons',\n",
       " 683: 'bus',\n",
       " 684: 'business',\n",
       " 685: 'businesses',\n",
       " 686: 'bussell',\n",
       " 687: 'busy',\n",
       " 688: 'but',\n",
       " 689: 'butter',\n",
       " 690: 'button',\n",
       " 691: 'buttons',\n",
       " 692: 'buy',\n",
       " 693: 'buyer',\n",
       " 694: 'buyerbe',\n",
       " 695: 'buyers',\n",
       " 696: 'buying',\n",
       " 697: 'buyit',\n",
       " 698: 'buzzing',\n",
       " 699: 'by',\n",
       " 700: 'bye',\n",
       " 701: 'ca42',\n",
       " 702: 'caballeros',\n",
       " 703: 'cable',\n",
       " 704: 'cables',\n",
       " 705: 'caesar',\n",
       " 706: 'cafe',\n",
       " 707: 'café',\n",
       " 708: 'cailles',\n",
       " 709: 'cakeohhh',\n",
       " 710: 'cakes',\n",
       " 711: 'calamari',\n",
       " 712: 'calendar',\n",
       " 713: 'california',\n",
       " 714: 'call',\n",
       " 715: 'called',\n",
       " 716: 'calligraphy',\n",
       " 717: 'callings',\n",
       " 718: 'calls',\n",
       " 719: 'came',\n",
       " 720: 'camelback',\n",
       " 721: 'cameo',\n",
       " 722: 'camera',\n",
       " 723: 'camerawork',\n",
       " 724: 'camp',\n",
       " 725: 'campy',\n",
       " 726: 'can',\n",
       " 727: 'canada',\n",
       " 728: 'canal',\n",
       " 729: 'cancan',\n",
       " 730: 'cancellation',\n",
       " 731: 'cancelling',\n",
       " 732: 'candace',\n",
       " 733: 'candle',\n",
       " 734: 'cando',\n",
       " 735: 'cannoli',\n",
       " 736: 'cant',\n",
       " 737: 'capability',\n",
       " 738: 'capacity',\n",
       " 739: 'cape',\n",
       " 740: 'capers',\n",
       " 741: 'captain',\n",
       " 742: 'captured',\n",
       " 743: 'captures',\n",
       " 744: 'car',\n",
       " 745: 'carbs',\n",
       " 746: 'card',\n",
       " 747: 'cardboard',\n",
       " 748: 'cardellini',\n",
       " 749: 'care',\n",
       " 750: 'careful',\n",
       " 751: 'caring',\n",
       " 752: 'carlys',\n",
       " 753: 'carol',\n",
       " 754: 'carpaccio',\n",
       " 755: 'carrell',\n",
       " 756: 'carried',\n",
       " 757: 'carriers',\n",
       " 758: 'carries',\n",
       " 759: 'carry',\n",
       " 760: 'cars',\n",
       " 761: 'cart',\n",
       " 762: 'cartel',\n",
       " 763: 'cartoon',\n",
       " 764: 'cartoons',\n",
       " 765: 'case',\n",
       " 766: 'cases',\n",
       " 767: 'cash',\n",
       " 768: 'cashew',\n",
       " 769: 'cashier',\n",
       " 770: 'casing',\n",
       " 771: 'casino',\n",
       " 772: 'cassette',\n",
       " 773: 'cast',\n",
       " 774: 'casted',\n",
       " 775: 'casting',\n",
       " 776: 'cat',\n",
       " 777: 'catching',\n",
       " 778: 'catchy',\n",
       " 779: 'caterpillar',\n",
       " 780: 'caught',\n",
       " 781: 'cause',\n",
       " 782: 'caused',\n",
       " 783: 'causing',\n",
       " 784: 'cavier',\n",
       " 785: 'cbr',\n",
       " 786: 'cds',\n",
       " 787: 'ceases',\n",
       " 788: 'celebration',\n",
       " 789: 'celebrity',\n",
       " 790: 'cell',\n",
       " 791: 'cellphone',\n",
       " 792: 'cellphones',\n",
       " 793: 'cellular',\n",
       " 794: 'celluloid',\n",
       " 795: 'center',\n",
       " 796: 'centers',\n",
       " 797: 'central',\n",
       " 798: 'cents',\n",
       " 799: 'century',\n",
       " 800: 'certain',\n",
       " 801: 'certainly',\n",
       " 802: 'cg',\n",
       " 803: 'cgi',\n",
       " 804: 'chai',\n",
       " 805: 'chains',\n",
       " 806: 'chalkboard',\n",
       " 807: 'challenges',\n",
       " 808: 'chance',\n",
       " 809: 'chanceit',\n",
       " 810: 'change',\n",
       " 811: 'changes',\n",
       " 812: 'changing',\n",
       " 813: 'channel',\n",
       " 814: 'char',\n",
       " 815: 'character',\n",
       " 816: 'characterage',\n",
       " 817: 'characterisation',\n",
       " 818: 'characters',\n",
       " 819: 'charcoal',\n",
       " 820: 'charge',\n",
       " 821: 'charged',\n",
       " 822: 'chargelife',\n",
       " 823: 'charger',\n",
       " 824: 'chargers',\n",
       " 825: 'charges',\n",
       " 826: 'charging',\n",
       " 827: 'charisma',\n",
       " 828: 'charismafree',\n",
       " 829: 'charismatic',\n",
       " 830: 'charles',\n",
       " 831: 'charlie',\n",
       " 832: 'charm',\n",
       " 833: 'charming',\n",
       " 834: 'chase',\n",
       " 835: 'chasing',\n",
       " 836: 'cheap',\n",
       " 837: 'cheaper',\n",
       " 838: 'cheaply',\n",
       " 839: 'cheapy',\n",
       " 840: 'cheated',\n",
       " 841: 'check',\n",
       " 842: 'checked',\n",
       " 843: 'checking',\n",
       " 844: 'cheek',\n",
       " 845: 'cheekbones',\n",
       " 846: 'cheerfull',\n",
       " 847: 'cheerless',\n",
       " 848: 'cheese',\n",
       " 849: 'cheeseburger',\n",
       " 850: 'cheesecurds',\n",
       " 851: 'cheesiness',\n",
       " 852: 'cheesy',\n",
       " 853: 'chef',\n",
       " 854: 'chefs',\n",
       " 855: 'chemistry',\n",
       " 856: 'chewy',\n",
       " 857: 'chick',\n",
       " 858: 'chicken',\n",
       " 859: 'chickens',\n",
       " 860: 'chickenwith',\n",
       " 861: 'child',\n",
       " 862: 'childhood',\n",
       " 863: 'childlike',\n",
       " 864: 'children',\n",
       " 865: 'childrens',\n",
       " 866: 'chills',\n",
       " 867: 'chilly',\n",
       " 868: 'chimplike',\n",
       " 869: 'china',\n",
       " 870: 'chinese',\n",
       " 871: 'chip',\n",
       " 872: 'chipolte',\n",
       " 873: 'chipotle',\n",
       " 874: 'chips',\n",
       " 875: 'chocolate',\n",
       " 876: 'chodorov',\n",
       " 877: 'choice',\n",
       " 878: 'choices',\n",
       " 879: 'choked',\n",
       " 880: 'choose',\n",
       " 881: 'chosen',\n",
       " 882: 'choux',\n",
       " 883: 'chow',\n",
       " 884: 'christmas',\n",
       " 885: 'christopher',\n",
       " 886: 'church',\n",
       " 887: 'cibo',\n",
       " 888: 'cinema',\n",
       " 889: 'cinematic',\n",
       " 890: 'cinematographers',\n",
       " 891: 'cinematography',\n",
       " 892: 'cinematographyif',\n",
       " 893: 'cingulair',\n",
       " 894: 'cingular',\n",
       " 895: 'cingularatt',\n",
       " 896: 'circumstances',\n",
       " 897: 'claimed',\n",
       " 898: 'clarity',\n",
       " 899: 'class',\n",
       " 900: 'classic',\n",
       " 901: 'classical',\n",
       " 902: 'classics',\n",
       " 903: 'classy',\n",
       " 904: 'classywarm',\n",
       " 905: 'clean',\n",
       " 906: 'clear',\n",
       " 907: 'clearer',\n",
       " 908: 'clearly',\n",
       " 909: 'clever',\n",
       " 910: 'clichés',\n",
       " 911: 'clicks',\n",
       " 912: 'clients',\n",
       " 913: 'cliff',\n",
       " 914: 'climax',\n",
       " 915: 'climbing',\n",
       " 916: 'clip',\n",
       " 917: 'clipping',\n",
       " 918: 'clips',\n",
       " 919: 'clock',\n",
       " 920: 'close',\n",
       " 921: 'closed',\n",
       " 922: 'closeup',\n",
       " 923: 'closeups',\n",
       " 924: 'clothes',\n",
       " 925: 'club',\n",
       " 926: 'clue',\n",
       " 927: 'coach',\n",
       " 928: 'coal',\n",
       " 929: 'coastal',\n",
       " 930: 'coaster',\n",
       " 931: 'cocktail',\n",
       " 932: 'cocktails',\n",
       " 933: 'coconut',\n",
       " 934: 'cod',\n",
       " 935: 'coffee',\n",
       " 936: 'coherent',\n",
       " 937: 'cold',\n",
       " 938: 'colder',\n",
       " 939: 'cole',\n",
       " 940: 'colleague',\n",
       " 941: 'collect',\n",
       " 942: 'collective',\n",
       " 943: 'college',\n",
       " 944: 'color',\n",
       " 945: 'colored',\n",
       " 946: 'colorful',\n",
       " 947: 'colors',\n",
       " 948: 'colours',\n",
       " 949: 'columbo',\n",
       " 950: 'combination',\n",
       " 951: 'combo',\n",
       " 952: 'combos',\n",
       " 953: 'come',\n",
       " 954: 'comedic',\n",
       " 955: 'comedy',\n",
       " 956: 'comes',\n",
       " 957: 'comfort',\n",
       " 958: 'comfortable',\n",
       " 959: 'comfortably',\n",
       " 960: 'comfortible',\n",
       " 961: 'comforting',\n",
       " 962: 'comical',\n",
       " 963: 'coming',\n",
       " 964: 'commands',\n",
       " 965: 'comment',\n",
       " 966: 'commentary',\n",
       " 967: 'commented',\n",
       " 968: 'comments',\n",
       " 969: 'commercial',\n",
       " 970: 'commercials',\n",
       " 971: 'common',\n",
       " 972: 'communicate',\n",
       " 973: 'communication',\n",
       " 974: 'communications',\n",
       " 975: 'community',\n",
       " 976: 'commuter',\n",
       " 977: 'companions',\n",
       " 978: 'company',\n",
       " 979: 'companys',\n",
       " 980: 'comparablypriced',\n",
       " 981: 'compared',\n",
       " 982: 'compelling',\n",
       " 983: 'compete',\n",
       " 984: 'competent',\n",
       " 985: 'competitors',\n",
       " 986: 'complain',\n",
       " 987: 'complained',\n",
       " 988: 'complaint',\n",
       " 989: 'complaints',\n",
       " 990: 'complete',\n",
       " 991: 'completed',\n",
       " 992: 'completely',\n",
       " 993: 'complex',\n",
       " 994: 'complexity',\n",
       " 995: 'compliments',\n",
       " 996: 'composed',\n",
       " 997: 'composition',\n",
       " 998: 'comprehensible',\n",
       " 999: 'compromise',\n",
       " 1000: 'computer',\n",
       " ...}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_int_dict, int_to_word_dict = create_dictionaries(sorted(vocab))\n",
    "int_to_word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 10.0 11.783666666666667\n"
     ]
    }
   ],
   "source": [
    "# decide on padding (max & mean length)\n",
    "lengths = [len(x) for x in reviews]\n",
    "print(np.max(lengths), np.median(lengths), np.mean(lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_text(tokenized_reviews, seq_length):\n",
    "    reviews = []\n",
    "    for review in tokenized_reviews:\n",
    "        if len(review) >= seq_length:\n",
    "            reviews.append(review[:seq_length])\n",
    "        else:\n",
    "            reviews.append(['']*(seq_length - len(review)) + review )\n",
    "    return np.array(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_sentences = pad_text(reviews, seq_length = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', 'one', 'more', 'thing', 'i', 'can', 'tolerate',\n",
       "       'political', 'incorrectness', 'very', 'well', 'im', 'all', 'for',\n",
       "       'artistic', 'freedom', 'and', 'suspension', 'of', 'disbelief',\n",
       "       'but', 'the', 'slavic', 'female', 'character', 'was', 'just',\n",
       "       'too', 'much'], dtype='<U33')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add empty token to dictionary\n",
    "word_to_int_dict[''] = 0\n",
    "int_to_word_dict[0] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now encode sentences\n",
    "encoded_sentences = np.array([[word_to_int_dict[word] for word in review ] for review in padded_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "       3272, 3059, 4752, 2364,  726, 4828, 3568, 2426, 5109, 5231, 2387,\n",
       "        204, 1908,  326, 1943,  245, 4629, 3244, 1366,  688, 4728, 4300,\n",
       "       1797,  815, 5187, 2600, 4839, 3094])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, n_vocab, n_embed, n_hidden, n_output, n_layers, drop_p=0.8):\n",
    "        super().__init__()\n",
    "        self.n_vocab = n_vocab\n",
    "        self.n_embed = n_embed\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "        self.lstm = nn.LSTM(n_embed, n_hidden, n_layers, batch_first = True, dropout = drop_p)\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "        self.fc = nn.Linear(n_hidden, n_output)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    \n",
    "    def forward(self, input_words):\n",
    "        \n",
    "        embedded_words = self.embedding(input_words)\n",
    "        lstm_out, h = self.lstm(embedded_words)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.n_hidden)\n",
    "        fc_out = self.fc(lstm_out)\n",
    "        sigmoid_out = self.sigmoid(fc_out)\n",
    "        sigmoid_out = sigmoid_out.view(batch_size, -1)\n",
    "        \n",
    "        sigmoid_last = sigmoid_out[:, -1]\n",
    "        \n",
    "        return sigmoid_last, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        device = \"cpu\"\n",
    "        weights = next(self.parameters()).data\n",
    "        h = (weights.new(self.n_layers, \n",
    "                         batch_size,\n",
    "                         self.n_hidden\n",
    "                        ).zero_().to(device),\n",
    "             weights.new(self.n_layers, \n",
    "                         batch_size,\n",
    "                         self.n_hidden\n",
    "                        ).zero_().to(device),\n",
    "            )\n",
    "        return h\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentLSTM(\n",
       "  (embedding): Embedding(5401, 50)\n",
       "  (lstm): LSTM(50, 100, num_layers=2, batch_first=True, dropout=0.8)\n",
       "  (dropout): Dropout(p=0.8, inplace=False)\n",
       "  (fc): Linear(in_features=100, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_vocab = len(word_to_int_dict)\n",
    "n_embed = 50\n",
    "n_hidden = 100\n",
    "n_output = 1\n",
    "n_layers = 2\n",
    "\n",
    "net = SentimentLSTM(n_vocab, n_embed, n_hidden, n_output, n_layers)\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_labels = np.array([int(x) for x in data['sentiment'].values])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.8\n",
    "valid_ratio = (1 - train_ratio) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = len(encoded_sentences)\n",
    "train_cutoff = int(total*train_ratio)\n",
    "valid_cutoff = int(total * (1 - valid_ratio))\n",
    "print(train_cutoff, valid_cutoff)\n",
    "\n",
    "train_x, train_y = torch.Tensor(encoded_sentences[:train_cutoff]).long(),               torch.Tensor(sentence_labels[:train_cutoff]).long()\n",
    "valid_x, valid_y = torch.Tensor(encoded_sentences[train_cutoff : valid_cutoff]).long(), torch.Tensor(sentence_labels[train_cutoff:valid_cutoff]).long()\n",
    "test_x, test_y   = torch.Tensor(encoded_sentences[valid_cutoff:]).long(),               torch.Tensor(sentence_labels[valid_cutoff:])\n",
    "\n",
    "train_data = TensorDataset(train_x, train_y)\n",
    "valid_data = TensorDataset(valid_x, valid_y)\n",
    "test_data  = TensorDataset(test_x, test_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 2400, 300, 300)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total, len(train_data), len(valid_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1 # single sentence\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "valid_loader = DataLoader(valid_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(test_data, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_every = 2400\n",
    "step = 0\n",
    "n_epochs = 5\n",
    "clip = 5\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/FrancescaSogaro/.pyenv/versions/3.8.2/lib/python3.8/site-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
      "<ipython-input-29-70e46d77c721>:10: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(net.parameters(), clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5 Step: 2400 Training Loss: 0.8810 Validation Loss: 0.6239\n",
      "Epoch: 2/5 Step: 4800 Training Loss: 1.1317 Validation Loss: 0.6526\n",
      "Epoch: 3/5 Step: 7200 Training Loss: 0.0112 Validation Loss: 0.6450\n",
      "Epoch: 4/5 Step: 9600 Training Loss: 0.0564 Validation Loss: 0.7117\n",
      "Epoch: 5/5 Step: 12000 Training Loss: 0.0079 Validation Loss: 0.7279\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(n_epochs):\n",
    "    h = net.init_hidden(batch_size)\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        step +=1\n",
    "        net.zero_grad()\n",
    "        output, h = net(inputs)\n",
    "        loss  =  criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (step % print_every) == 0:\n",
    "            net.eval()\n",
    "            valid_losses = []\n",
    "            \n",
    "            for v_inputs, v_labels in valid_loader:\n",
    "                v_output, v_h = net(v_inputs)\n",
    "                v_loss = criterion(v_output.squeeze(), v_labels.float())\n",
    "                valid_losses.append(v_loss.item())\n",
    "                \n",
    "            print(\"Epoch: {}/{}\".format((epoch+1), n_epochs),\n",
    "                  \"Step: {}\".format(step),\n",
    "                  \"Training Loss: {:.4f}\".format(loss.item()),\n",
    "                  \"Validation Loss: {:.4f}\".format(np.mean(valid_losses)))\n",
    "            \n",
    "            net.train()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = SentimentLSTM(n_vocab, n_embed, n_hidden, n_output, n_layers)\n",
    "net.load_state_dict(torch.load('model.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss 1.0144421436911215\n",
      "test accuracy 0.7533333333333333\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "test_losses = []\n",
    "num_correct = 0\n",
    "\n",
    "for inputs, labels in  test_loader:\n",
    "    \n",
    "    test_output, test_h = net(inputs)\n",
    "    loss = criterion(test_output, labels.float())\n",
    "    test_losses.append(loss.item())\n",
    "    preds =  torch.round(test_output.squeeze())\n",
    "    correct_tensor = preds.eq(labels.float().view_as(preds))\n",
    "    correct = np.squeeze(correct_tensor.numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "print(f\"test loss {np.mean(test_losses)}\")\n",
    "print(f\"test accuracy {num_correct / len(test_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_review(review):\n",
    "    seq_length = 50\n",
    "    review.translate(str.maketrans('', '', punctuation)).lower().rstrip()\n",
    "    tokenized  = word_tokenize(review)\n",
    "    if len(tokenized) >= 50:\n",
    "        review = tokenized[:seq_length]\n",
    "    else:\n",
    "        review = ['']*(seq_length - len(tokenized)) + tokenized \n",
    "        \n",
    "    final  = []\n",
    "    for token in review:\n",
    "        try:\n",
    "            final.append(word_to_int_dict[token])\n",
    "        except:\n",
    "            final.append(word_to_int_dict['']) # if new word\n",
    "            \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(review):\n",
    "    net.eval()\n",
    "    words = np.array([preprocess_review(review)])\n",
    "    padded_words = torch.from_numpy(words)\n",
    "    pred_loader = DataLoader(padded_words, batch_size=1, shuffle=True)\n",
    "    for x in pred_loader:\n",
    "        output = net(x)[0].item()\n",
    "    msg = \"Positive rev.\" if output > 0.5 else \"Negative rev.\"\n",
    "    print(msg)\n",
    "    print(f\"prediction: {output}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive rev.\n",
      "prediction: 0.9965888261795044\n"
     ]
    }
   ],
   "source": [
    "predict(\"The film was good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative rev.\n",
      "prediction: 0.02365453727543354\n"
     ]
    }
   ],
   "source": [
    "predict(\"The film was bad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

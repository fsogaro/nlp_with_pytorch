{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "import itertools\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\\tWell, I thought we'd start with pronunciation, if that's okay with you.\\n\" \n",
      "\n",
      "b\"Well, I thought we'd start with pronunciation, if that's okay with you.\\tNot the hacking and gagging and spitting part.  Please.\\n\" \n",
      "\n",
      "b\"Not the hacking and gagging and spitting part.  Please.\\tOkay... then how 'bout we try out some French cuisine.  Saturday?  Night?\\n\" \n",
      "\n",
      "b\"You're asking me out.  That's so cute. What's your name again?\\tForget it.\\n\" \n",
      "\n",
      "b\"No, no, it's my fault -- we didn't have a proper introduction ---\\tCameron.\\n\" \n",
      "\n",
      "b\"Cameron.\\tThe thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\\n\" \n",
      "\n",
      "b\"The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\\tSeems like she could get a date easy enough...\\n\" \n",
      "\n",
      "b'Why?\\tUnsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\\n' \n",
      "\n",
      "b\"Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\\tThat's a shame.\\n\" \n",
      "\n",
      "b'Gosh, if only we could find Kat a boyfriend...\\tLet me see what I can do.\\n' \n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus = \"movie_corpus\"\n",
    "corpus_name = \"movie_corpus\"\n",
    "datafile = os.path.join(\"..\", \"data\", corpus,  \"formatted_movie_lines.txt\")\n",
    "datafile\n",
    "with open(datafile, \"rb\") as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines[:10]:\n",
    "        print(str(line), \"\\n\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "\n",
    "class Vocabulary():\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {\n",
    "            PAD_token: \"PAD\",\n",
    "            SOS_token: \"SOS\",\n",
    "            EOS_token: \"EOS\",\n",
    "        }\n",
    "        self.num_words = 3\n",
    "        \n",
    "    def addWord(self, w):\n",
    "        if w not in self.word2index:\n",
    "            self.word2index[w] = self.num_words\n",
    "            self.word2count[w] = 1\n",
    "            self.index2word[self.num_words] = w\n",
    "            \n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[w] += 1\n",
    "            \n",
    "            \n",
    "    def addSentence(self, sent):\n",
    "        for word in sent.split(\" \"):\n",
    "            self.addWord(word)\n",
    "            \n",
    "    def trim(self, min_cnt):\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed = True\n",
    "        words_to_keep = []\n",
    "        for k, v in self.word2count.items():\n",
    "            if v >=  min_cnt:\n",
    "                words_to_keep.append(k)\n",
    "                \n",
    "        # re build       \n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {\n",
    "            PAD_token: \"PAD\",\n",
    "            SOS_token: \"SOS\",\n",
    "            EOS_token: \"EOS\",\n",
    "        }\n",
    "        self.num_words = 3\n",
    "        \n",
    "        for w in words_to_keep:\n",
    "            self.addWord(w)\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    return  ''.join(c for c in unicodedata.normalize('NFD', s) \n",
    "                   if unicodedata.category(c) != 'Mn')\n",
    "def cleanString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return  s\n",
    "\n",
    "def readVocs(datafile, corpus_name):\n",
    "    lines  = open(datafile,\n",
    "                  encoding = 'utf-8'\n",
    "                 ).read().strip().split(\"\\n\")\n",
    "    pairs = [[cleanString(s) for s in l.split(\"\\t\")] \n",
    "             for l in lines]\n",
    "    \n",
    "    voc = Vocabulary(corpus_name)\n",
    "    return voc, pairs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sake of training, use only short sentences\n",
    "def filterPair(p, max_length): \n",
    "    return len(p[0].split(\" \")) < max_length and len(p[1].split(\" \")) < max_length \n",
    "\n",
    "def filterPairs(pairs, max_length): \n",
    "    return [pair for pair in pairs if filterPair(pair, max_length)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221282  Sentence pairs\n",
      "64271  Sentence pairs after trimming\n",
      "18008  distinct words in vocabilary\n"
     ]
    }
   ],
   "source": [
    "def loadData(corpus, corpus_name, datafile, max_length):\n",
    "    voc, pairs = readVocs(datafile, corpus_name)\n",
    "    print(f\"{len(pairs)}  Sentence pairs\")\n",
    "    pairs = filterPairs(pairs, max_length)\n",
    "    print(f\"{len(pairs)}  Sentence pairs after trimming\")\n",
    "    \n",
    "    for p in pairs:\n",
    "        voc.addSentence(p[0])\n",
    "        voc.addSentence(p[1])\n",
    "    print(f\"{voc.num_words}  distinct words in vocabilary\")\n",
    "    return voc, pairs\n",
    "\n",
    "max_length = 10\n",
    "voc, pairs = loadData(corpus, corpus_name, datafile, max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example pairs\n",
      "['four', 'three minutes to go !']\n",
      "['three minutes to go !', 'yes .']\n",
      "['another fifteen seconds to go .', 'do something ! stall them !']\n",
      "['yes sir name please ?', 'food !']\n",
      "['food !', 'do you have a reservation ?']\n",
      "['do you have a reservation ?', 'food ! !']\n",
      "['grrrhmmnnnjkjmmmnn !', 'franz ! help ! lunatic !']\n",
      "['what o clock is it mr noggs ?', 'eleven o clock my lorj']\n",
      "['stuart ?', 'yes .']\n",
      "['yes .', 'how quickly can you move your artillery forward ?']\n"
     ]
    }
   ],
   "source": [
    "print(\"Example pairs\")\n",
    "for pair in pairs[-10:]:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  remove rare words, so that we reduce complexity and less time to train, given  that we don't have a big dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeRareWords(voc, all_pairs, minimum):\n",
    "    voc.trim(minimum)\n",
    "    pairs_to_keep = []\n",
    "    for p in all_pairs:\n",
    "        keep = True\n",
    "        for word in p[0].split(\" \"):\n",
    "            if word not in voc.word2index:\n",
    "                keep = False\n",
    "                break\n",
    "        for word in p[1].split(\" \"):\n",
    "            if word not in voc.word2index:\n",
    "                keep = False\n",
    "                break\n",
    "        if keep:\n",
    "            pairs_to_keep.append(p)\n",
    "    print(f\"Trimmed from {len(all_pairs)} pairs to {len(pairs_to_keep)} {100*len(pairs_to_keep)/len(all_pairs)}\")\n",
    "    return  pairs_to_keep\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimmed from 64271 pairs to 53165 82.72004481025657\n"
     ]
    }
   ],
   "source": [
    "minimum_count = 3\n",
    "pairs = removeRareWords(voc, pairs, minimum_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform sentence pairs to vecotrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexFromSentence(voc, sent):\n",
    "    return [voc.word2index[word] for word in sent.split(' ')] + [EOS_token]\n",
    "\n",
    "def zeroPad(l, fillvalue=PAD_token):\n",
    "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
    "\n",
    "def inputVar(l, voc):\n",
    "    indexes_batch = [indexFromSentence(voc, sentence) for sentence in l]\n",
    "    padList = zeroPad(indexes_batch)\n",
    "    padTensor = torch.LongTensor(padList)\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    return padTensor, indexes_batch\n",
    "\n",
    "def getMask(l, value=PAD_token):\n",
    "    # we don not want to train the modell on padded tokes (those that serve to  make input the same length)\n",
    "    # so mask them\n",
    "    \n",
    "    m = []\n",
    "    for i, seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if token == value:\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "    return m\n",
    "\n",
    "def outputVar(l, voc):\n",
    "    indexes_batch = [indexFromSentence(voc, sentence) for sentence in l]\n",
    "    max_target_length = max([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPad(indexes_batch)\n",
    "    mask = torch.BoolTensor(getMask(padList))\n",
    "    padTensor = torch.LongTensor(padList)\n",
    "    return padTensor, mask, max_target_length\n",
    "\n",
    "\n",
    "def batch2Train(voc, batch):\n",
    "    \n",
    "    batch.sort(key = lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "    input_batch = []\n",
    "    output_batch = []\n",
    "\n",
    "    for p in  batch:\n",
    "        input_batch.append(p[0])\n",
    "        output_batch.append(p[1])\n",
    "\n",
    "    inp, length = inputVar(input_batch, voc)\n",
    "    output, mask, max_target_length = outputVar(output_batch, voc)\n",
    "    return inp, length, output, mask, max_target_length\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "tensor([[  25,  167,   25,   92,   47],\n",
      "        [ 260,    4,   68,    7,    7],\n",
      "        [  76,   25,  746,  192,    6],\n",
      "        [ 116,  534,  174,    6,    2],\n",
      "        [ 117,   76,   66,    2,    0],\n",
      "        [1434,    4,    2,    0,    0],\n",
      "        [   7,    2,    0,    0,    0],\n",
      "        [   4,    0,    0,    0,    0],\n",
      "        [   2,    0,    0,    0,    0]])\n",
      "Target:\n",
      "tensor([[1434,   60,   50,  387,  281],\n",
      "        [  83,    4,   47,   25,    7],\n",
      "        [   6,    2,    7,  192,   14],\n",
      "        [   2,    0,  260,    6,  159],\n",
      "        [   0,    0,    6,    2,    4],\n",
      "        [   0,    0,    2,    0,    2]])\n",
      "Mask:\n",
      "tensor([[ True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True],\n",
      "        [ True, False,  True,  True,  True],\n",
      "        [False, False,  True,  True,  True],\n",
      "        [False, False,  True, False,  True]])\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "test_batch_size = 5\n",
    "batches = batch2Train(voc, [random.choice(pairs) for _ in range(test_batch_size)])\n",
    "\n",
    "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
    "print(\"Input:\")\n",
    "print(input_variable)\n",
    "\n",
    "print(\"Target:\")\n",
    "print(target_variable)\n",
    "\n",
    "print(\"Mask:\")\n",
    "print(mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "        \n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
    "                          dropout=(0 if n_layers ==1 else dropout),\n",
    "                          bidirectional=True)\n",
    "        \n",
    "        \n",
    "        def forward(self, input_seq, input_lengths, hidden=None):\n",
    "            embedded = self.embedding(input_seq)\n",
    "            # packed so all inputs are of same length\n",
    "            packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "            outputs, hidden = self.gru(packed, hidden)\n",
    "            # upack padding and sum the GRU outputs\n",
    "            outputs , _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "            outputs = outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:]\n",
    "            return outputs, hidden\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build  attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, hidden_state):\n",
    "        super(Attn, self).__init__()\n",
    "        self.hidden_state = hidden_state\n",
    "        \n",
    "    def dot_score(self, hidden, encoder_outputs):\n",
    "        return torch.sum(hidden * encoder_outputs, dim=2)\n",
    "    \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        attn_energies = self.dot_score(hidden, encoder_outputs)\n",
    "        attn_energies = attn_energies.t()\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        self.embedding = embedding\n",
    "        \n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
    "                          dropout=(0 if n_layers ==1 else dropout),\n",
    "                          bidirectional=False) # don't make it bi-directional\n",
    "        self.concat = nn.Linear(2*hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.attn = Attn(hidden_size)\n",
    "        \n",
    "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input_step)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        rnn_output, hidden  = self.gru(embedded, last_hidden)\n",
    "        # get attention weights\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0,1))\n",
    "        # concatenate weighted context vector with output from gru\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output =  torch.tanh(self.concat(concat_input))\n",
    "        # use concatenated output to predict the next word using softmax\n",
    "        output = self.out(concat_output)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        \n",
    "        return output, hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  first define the  measure of loss of the models ( there are the padded tokens whihc should not count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLLMaskLoss(inp, target, mask):\n",
    "    TotalN = mask.sum()\n",
    "    CELoss = torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
    "    loss = CELoss.mask_select(mask).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, TotalN.item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_variable, lengths, target_variable,\n",
    "          mask, max_target_length, encoder, decoder,\n",
    "          embedding, encoder_optimizer, decoder_optimizer,\n",
    "          batch_size, clip, max_length=max_length):\n",
    "    \n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    input_variable =  input_variable.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    target_variable = target_variable.to(device)\n",
    "    mask = mask.to(device)\n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "    \n",
    "    # forward pass of inputs to encoder and get hidden state\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "    # create initial decoder input starting with SOS token\n",
    "    # then set hidden state of decoder to  be equal to the one of the encoder\n",
    "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
    "    decoder_input = decoder_input.to(device)\n",
    "    \n",
    "    decoder_hidden =  encoder_hidden[:decoder.n_layers]\n",
    "    \n",
    "    # use teahcer forcing\n",
    "    use_TF  =  True if random.random() < teacher_forcing_ration else False\n",
    "    \n",
    "    if use_TF:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input,\n",
    "                                                     decoder_hidden,\n",
    "                                                     encoder_outputs)\n",
    "            decoder_input = target_variable[t].view(1, -1)\n",
    "            mask_loss, nTotal = NLLMaskLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item()*nTotal)\n",
    "            n_totals +=nTotal\n",
    "    else:\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input,\n",
    "                                                     decoder_hidden,\n",
    "                                                     encoder_outputs)\n",
    "        _, topi = decoder_output.topk(1)\n",
    "        decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
    "        decoder_input = decoder_input.to(device)\n",
    "        mask_loss, nTotal = NLLMaskLoss(decoder_output, target_variable[t], mask[t])\n",
    "        loss += mask_loss\n",
    "        print_losses.append(mask_loss.item()*nTotal)\n",
    "        n_totals +=nTotal\n",
    "        \n",
    "    # do backprop, gradient cliping, update weights\n",
    "    loss.backward()\n",
    "    _ =  nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    _ =  nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return sum(print_losses)/n_totals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# need to create train callers on batches\n",
    "\n",
    "# split data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(model_name, voc, pairs, encoder, decoder,\n",
    "               encoder_optimizer, decoder_optimizer,\n",
    "              embedding, encoder_n_layers, decoder_n_layers, save_dir, \n",
    "              n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename):\n",
    "    \n",
    "    training_batches = [batch2Train(voc, [random.choice(pairs) for _ in range(batch_size)])\n",
    "                      for _ in range(n_iteration)]\n",
    "                       \n",
    "    print(\"starting...\")\n",
    "    start_iteration = 1\n",
    "    print_loss = 0\n",
    "    if loadFilename:\n",
    "        start_iteration = checkpoint['iteration']+1\n",
    "    print(\"beginning training...\")\n",
    "    for iteration in range(start_iteration, n_iteration+1):\n",
    "        trainig_batch = trainig_batches[iteartion-1]\n",
    "        input_variable, lengths, target_variable, mask, max_target_length = trainig_batch\n",
    "        loss = train(input_variable, lengths, target_variable, mask, max_target_length,\n",
    "                    encoder, decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
    "        print_loss +=loss\n",
    "        \n",
    "    \n",
    "        \n",
    "        if iteration % print_every == 0:\n",
    "            print_loss_avg = print_loss / print_every\n",
    "            print(\"Iteration: {}; Percent done: {:.1f}%; Mean loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n",
    "            print_loss = 0\n",
    "\n",
    "        if (iteration % save_every == 0):\n",
    "            directory = os.path.join(save_dir, model_name, corpus_name, '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size))\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            torch.save({\n",
    "                'iteration': iteration,\n",
    "                'en': encoder.state_dict(),\n",
    "                'de': decoder.state_dict(),\n",
    "                'en_opt': encoder_optimizer.state_dict(),\n",
    "                'de_opt': decoder_optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                'voc_dict': voc.__dict__,\n",
    "                'embedding': embedding.state_dict()\n",
    "            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation is trycky as the is no exact asnwers, many are valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedySearchDencoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(GreedySearchDencoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "        \n",
    "    def forward(self, input_seq, input_length, max_length):\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "        \n",
    "        # create decoder input wiw SOS tokens and initiallize tensor to  append words\n",
    "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
    "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
    "        all_scores = torch.zeros([0], device=device)\n",
    "        # iterate decoding one word at a time, use max to get highest prediced word\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
    "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
    "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
    "            # output becomes next iter input\n",
    "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "        return all_tokens, all_scores\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, searcher, voc, sentence, max_length = max_length):\n",
    "    indices = [indexFromSentence(voc, sentence)]\n",
    "    lengths = torch.tensor([len()])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

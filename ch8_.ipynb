{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "import itertools\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\\tWell, I thought we'd start with pronunciation, if that's okay with you.\\n\" \n",
      "\n",
      "b\"Well, I thought we'd start with pronunciation, if that's okay with you.\\tNot the hacking and gagging and spitting part.  Please.\\n\" \n",
      "\n",
      "b\"Not the hacking and gagging and spitting part.  Please.\\tOkay... then how 'bout we try out some French cuisine.  Saturday?  Night?\\n\" \n",
      "\n",
      "b\"You're asking me out.  That's so cute. What's your name again?\\tForget it.\\n\" \n",
      "\n",
      "b\"No, no, it's my fault -- we didn't have a proper introduction ---\\tCameron.\\n\" \n",
      "\n",
      "b\"Cameron.\\tThe thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\\n\" \n",
      "\n",
      "b\"The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\\tSeems like she could get a date easy enough...\\n\" \n",
      "\n",
      "b'Why?\\tUnsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\\n' \n",
      "\n",
      "b\"Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\\tThat's a shame.\\n\" \n",
      "\n",
      "b'Gosh, if only we could find Kat a boyfriend...\\tLet me see what I can do.\\n' \n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus = \"movie_corpus\"\n",
    "corpus_name = \"movie_corpus\"\n",
    "datafile = os.path.join(\"..\", \"data\", corpus,  \"formatted_movie_lines.txt\")\n",
    "datafile\n",
    "with open(datafile, \"rb\") as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines[:10]:\n",
    "        print(str(line), \"\\n\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "\n",
    "class Vocabulary():\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {\n",
    "            PAD_token: \"PAD\",\n",
    "            SOS_token: \"SOS\",\n",
    "            EOS_token: \"EOS\",\n",
    "        }\n",
    "        self.num_words = 3\n",
    "        \n",
    "    def addWord(self, w):\n",
    "        if w not in self.word2index:\n",
    "            self.word2index[w] = self.num_words\n",
    "            self.word2count[w] = 1\n",
    "            self.index2word[self.num_words] = w\n",
    "            \n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[w] += 1\n",
    "            \n",
    "            \n",
    "    def addSentence(self, sent):\n",
    "        for word in sent.split(\" \"):\n",
    "            self.addWord(word)\n",
    "            \n",
    "    def trim(self, min_cnt):\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed = True\n",
    "        words_to_keep = []\n",
    "        for k, v in self.word2count.items():\n",
    "            if v >=  min_cnt:\n",
    "                words_to_keep.append(k)\n",
    "                \n",
    "        # re build       \n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {\n",
    "            PAD_token: \"PAD\",\n",
    "            SOS_token: \"SOS\",\n",
    "            EOS_token: \"EOS\",\n",
    "        }\n",
    "        self.num_words = 3\n",
    "        \n",
    "        for w in words_to_keep:\n",
    "            self.addWord(w)\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    return  ''.join(c for c in unicodedata.normalize('NFD', s) \n",
    "                   if unicodedata.category(c) != 'Mn')\n",
    "def cleanString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return  s\n",
    "\n",
    "def readVocs(datafile, corpus_name):\n",
    "    lines  = open(datafile,\n",
    "                  encoding = 'utf-8'\n",
    "                 ).read().strip().split(\"\\n\")\n",
    "    pairs = [[cleanString(s) for s in l.split(\"\\t\")] \n",
    "             for l in lines]\n",
    "    \n",
    "    voc = Vocabulary(corpus_name)\n",
    "    return voc, pairs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sake of training, use only short sentences\n",
    "def filterPair(p, max_length): \n",
    "    return len(p[0].split(\" \")) < max_length and len(p[1].split(\" \")) < max_length \n",
    "\n",
    "def filterPairs(pairs, max_length): \n",
    "    return [pair for pair in pairs if filterPair(pair, max_length)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221282  Sentence pairs\n",
      "64271  Sentence pairs after trimming\n",
      "18008  distinct words in vocabilary\n"
     ]
    }
   ],
   "source": [
    "def loadData(corpus, corpus_name, datafile, max_length):\n",
    "    voc, pairs = readVocs(datafile, corpus_name)\n",
    "    print(f\"{len(pairs)}  Sentence pairs\")\n",
    "    pairs = filterPairs(pairs, max_length)\n",
    "    print(f\"{len(pairs)}  Sentence pairs after trimming\")\n",
    "    \n",
    "    for p in pairs:\n",
    "        voc.addSentence(p[0])\n",
    "        voc.addSentence(p[1])\n",
    "    print(f\"{voc.num_words}  distinct words in vocabilary\")\n",
    "    return voc, pairs\n",
    "\n",
    "max_length = 10\n",
    "voc, pairs = loadData(corpus, corpus_name, datafile, max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example pairs\n",
      "['four', 'three minutes to go !']\n",
      "['three minutes to go !', 'yes .']\n",
      "['another fifteen seconds to go .', 'do something ! stall them !']\n",
      "['yes sir name please ?', 'food !']\n",
      "['food !', 'do you have a reservation ?']\n",
      "['do you have a reservation ?', 'food ! !']\n",
      "['grrrhmmnnnjkjmmmnn !', 'franz ! help ! lunatic !']\n",
      "['what o clock is it mr noggs ?', 'eleven o clock my lorj']\n",
      "['stuart ?', 'yes .']\n",
      "['yes .', 'how quickly can you move your artillery forward ?']\n"
     ]
    }
   ],
   "source": [
    "print(\"Example pairs\")\n",
    "for pair in pairs[-10:]:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  remove rare words, so that we reduce complexity and less time to train, given  that we don't have a big dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeRareWords(voc, all_pairs, minimum):\n",
    "    voc.trim(minimum)\n",
    "    pairs_to_keep = []\n",
    "    for p in all_pairs:\n",
    "        keep = True\n",
    "        for word in p[0].split(\" \"):\n",
    "            if word not in voc.word2index:\n",
    "                keep = False\n",
    "                break\n",
    "        for word in p[1].split(\" \"):\n",
    "            if word not in voc.word2index:\n",
    "                keep = False\n",
    "                break\n",
    "        if keep:\n",
    "            pairs_to_keep.append(p)\n",
    "    print(f\"Trimmed from {len(all_pairs)} pairs to {len(pairs_to_keep)} {100*len(pairs_to_keep)/len(all_pairs)}\")\n",
    "    return  pairs_to_keep\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimmed from 64271 pairs to 53165 82.72004481025657\n"
     ]
    }
   ],
   "source": [
    "minimum_count = 3\n",
    "pairs = removeRareWords(voc, pairs, minimum_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform sentence pairs to vecotrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexFromSentence(voc, sentence):\n",
    "    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n",
    "\n",
    "def zeroPad(l, fillvalue=PAD_token):\n",
    "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
    "\n",
    "def inputVar(l, voc):\n",
    "    indexes_batch = [indexFromSentence(voc, sentence) for sentence in l]\n",
    "    padList = zeroPad(indexes_batch)\n",
    "    padTensor = torch.LongTensor(padList)\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    return padTensor, indexes_batch\n",
    "\n",
    "def getMask(l, value=PAD_token):\n",
    "    # we don not want to train the modell on padded tokes (those that serve to  make input the same length)\n",
    "    # so mask them\n",
    "    \n",
    "    m = []\n",
    "    for i, seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if token == value:\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "    return m\n",
    "\n",
    "def outputVar(l, voc):\n",
    "    indexes_batch = [indexFromSentence(voc, sentence) for sentence in l]\n",
    "    max_target_length = max([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPad(indexes_batch)\n",
    "    mask = torch.BoolTensor(getMask(padList))\n",
    "    padTensor = torch.LongTensor(padList)\n",
    "    return padTensor, mask, max_target_length\n",
    "\n",
    "\n",
    "def batch2Train(voc, batch):\n",
    "    \n",
    "    batch.sort(key = lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "    input_batch = []\n",
    "    output_batch = []\n",
    "\n",
    "    for p in  batch:\n",
    "        input_batch.append(p[0])\n",
    "        output_batch.append(p[1])\n",
    "\n",
    "    inp, length = inputVar(input_batch, voc)\n",
    "    output, mask, max_target_length = outputVar(output_batch, voc)\n",
    "    return inp, length, output, mask, max_target_length\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  50,  242,   67, 5019,  158],\n",
       "        [  47,  188,  188,    4,   21],\n",
       "        [   7,   83,   38,    4,   66],\n",
       "        [ 118,    4,    4,    4,    2],\n",
       "        [  40,    4,    2,    2,    0],\n",
       "        [  84,    4,    0,    0,    0],\n",
       "        [   6,    2,    0,    0,    0],\n",
       "        [   2,    0,    0,    0,    0]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batch_size = 5\n",
    "batches = batch2Train(voc, [random.choice(pairs) for _ in range(test_batch_size)])\n",
    "\n",
    "input_variable, lengths, output_variable, mask, max_target_len = batches\n",
    "input_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

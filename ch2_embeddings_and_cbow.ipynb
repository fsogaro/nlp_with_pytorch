{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore glove word emebddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load glove vectors from file\n",
    "def load_glove(file_path):\n",
    "    file_object  = open(file_path, \"r\") \n",
    "    model = {} \n",
    "    for l in file_object:\n",
    "        line = l.split()\n",
    "        word = line[0]\n",
    "        value = np.array([float(val) for val in line[1:]])\n",
    "        model[word] = value\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_with_path = \"../data/embeddings/glove.6B.50d.txt\"\n",
    "glove = load_glove(file_with_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5897  , -0.55043 , -1.0106  ,  0.41226 ,  0.57348 ,  0.23464 ,\n",
       "        -0.35773 , -1.78    ,  0.10745 ,  0.74913 ,  0.45013 ,  1.0351  ,\n",
       "         0.48348 ,  0.47954 ,  0.51908 , -0.15053 ,  0.32474 ,  1.0789  ,\n",
       "        -0.90894 ,  0.42943 , -0.56388 ,  0.69961 ,  0.13501 ,  0.16557 ,\n",
       "        -0.063592,  0.35435 ,  0.42819 ,  0.1536  , -0.47018 , -1.0935  ,\n",
       "         1.361   , -0.80821 , -0.674   ,  1.2606  ,  0.29554 ,  1.0835  ,\n",
       "         0.2444  , -1.1877  , -0.60203 , -0.068315,  0.66256 ,  0.45336 ,\n",
       "        -1.0178  ,  0.68267 , -0.20788 , -0.73393 ,  1.2597  ,  0.15425 ,\n",
       "        -0.93256 , -0.15025 ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove['python'].reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.75120749]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calulclate cosine similarity (import from sklearn for example)\n",
    "cosine_similarity(glove['fast'].reshape(1, -1), glove['speed'].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.85888392]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embeddings operations -> classic queen - woman + man =  king \n",
    "predicted_king = glove['queen'] - glove['woman'] + glove['man']\n",
    "actual_king = glove['king']\n",
    "\n",
    "cosine_similarity(actual_king.reshape(1, -1), predicted_king.reshape(1, -1)) # quite close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculate own embeddings with CBOW (alternative would be skipgrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"How that personage haunted my dreams, I need scarcely tell you. On\n",
    "stormy nights, when the wind shook the four corners of the house and\n",
    "the surf roared along the cove and up the cliffs, I would see him in a\n",
    "thousand forms, and with a thousand diabolical expressions. Now the leg\n",
    "would be cut off at the knee, now at the hip, now he was a monstrous\n",
    "kind of a creature who had never had but the one leg, and that in the\n",
    "middle of his body. To see him leap and run and pursue me over hedge and\n",
    "ditch was the worst of nightmares. And altogether I paid pretty dear for\n",
    "my monthly fourpenny piece, in the shape of these abominable fancies\"\"\"\n",
    "\n",
    "text = text.replace(',','').replace('.','').lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus  = set(text)\n",
    "corpus_length  = len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {}\n",
    "inverse_word_dict = {}\n",
    "\n",
    "for i, word in enumerate(corpus):\n",
    "    word_dict[word] = i\n",
    "    inverse_word_dict[i] = word\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for i in range(2, len(text)-2):\n",
    "    sentence = [text[i-2], text[i-1], text[i+1], text[i+2]]\n",
    "    target = text[i]\n",
    "    data.append((sentence, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'how'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 20\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, corpus_length, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        \n",
    "        self.embeddings = nn.Embedding(corpus_length, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, 64)\n",
    "        self.linear2 = nn.Linear(64, corpus_length)\n",
    "        \n",
    "        self.activation_fun1 = nn.ReLU()\n",
    "        self.activation_fun2 = nn.LogSoftmax(dim = -1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        embeds = sum(self.embeddings(inputs)).view(1, -1)\n",
    "        out = self.linear1(embeds)\n",
    "        out = self.activation_fun1(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.activation_fun2(out)\n",
    "        return out\n",
    "        \n",
    "    def get_word_embedding(self, word):\n",
    "        word = torch.LongTensor([word_dict[word]])\n",
    "        embd = self.embeddings(word)\n",
    "        return embd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CBOW(corpus_length, embedding_size)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sentence_vector(sentence, word_dict):\n",
    "    idxs = [word_dict[w] for w in sentence]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([39, 38, 72,  2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_sentence_vector(data[0][0], word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict['personage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CBOW(\n",
       "  (embeddings): Embedding(82, 20)\n",
       "  (linear1): Linear(in_features=20, out_features=64, bias=True)\n",
       "  (linear2): Linear(in_features=64, out_features=82, bias=True)\n",
       "  (activation_fun1): ReLU()\n",
       "  (activation_fun2): LogSoftmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/100 loss: 536.54443359375\n",
      "Epoch 1/100 loss: 476.8662414550781\n",
      "Epoch 2/100 loss: 434.998291015625\n",
      "Epoch 3/100 loss: 396.72979736328125\n",
      "Epoch 4/100 loss: 358.97467041015625\n",
      "Epoch 5/100 loss: 320.5982360839844\n",
      "Epoch 6/100 loss: 281.8330993652344\n",
      "Epoch 7/100 loss: 243.61477661132812\n",
      "Epoch 8/100 loss: 207.54013061523438\n",
      "Epoch 9/100 loss: 174.24093627929688\n",
      "Epoch 10/100 loss: 144.42483520507812\n",
      "Epoch 11/100 loss: 118.19700622558594\n",
      "Epoch 12/100 loss: 96.01138305664062\n",
      "Epoch 13/100 loss: 77.28784942626953\n",
      "Epoch 14/100 loss: 62.22464370727539\n",
      "Epoch 15/100 loss: 50.232025146484375\n",
      "Epoch 16/100 loss: 40.90771484375\n",
      "Epoch 17/100 loss: 33.71605682373047\n",
      "Epoch 18/100 loss: 28.241331100463867\n",
      "Epoch 19/100 loss: 23.956239700317383\n",
      "Epoch 20/100 loss: 20.62024688720703\n",
      "Epoch 21/100 loss: 17.956451416015625\n",
      "Epoch 22/100 loss: 15.83503246307373\n",
      "Epoch 23/100 loss: 14.106616973876953\n",
      "Epoch 24/100 loss: 12.667831420898438\n",
      "Epoch 25/100 loss: 11.485947608947754\n",
      "Epoch 26/100 loss: 10.471734046936035\n",
      "Epoch 27/100 loss: 9.621793746948242\n",
      "Epoch 28/100 loss: 8.887165069580078\n",
      "Epoch 29/100 loss: 8.253457069396973\n",
      "Epoch 30/100 loss: 7.694531440734863\n",
      "Epoch 31/100 loss: 7.19869327545166\n",
      "Epoch 32/100 loss: 6.760395526885986\n",
      "Epoch 33/100 loss: 6.373589992523193\n",
      "Epoch 34/100 loss: 6.019771099090576\n",
      "Epoch 35/100 loss: 5.706351280212402\n",
      "Epoch 36/100 loss: 5.417640686035156\n",
      "Epoch 37/100 loss: 5.156655311584473\n",
      "Epoch 38/100 loss: 4.916329383850098\n",
      "Epoch 39/100 loss: 4.6993865966796875\n",
      "Epoch 40/100 loss: 4.497968673706055\n",
      "Epoch 41/100 loss: 4.310667037963867\n",
      "Epoch 42/100 loss: 4.139634609222412\n",
      "Epoch 43/100 loss: 3.9791455268859863\n",
      "Epoch 44/100 loss: 3.830256462097168\n",
      "Epoch 45/100 loss: 3.692843198776245\n",
      "Epoch 46/100 loss: 3.563413143157959\n",
      "Epoch 47/100 loss: 3.4412057399749756\n",
      "Epoch 48/100 loss: 3.328336000442505\n",
      "Epoch 49/100 loss: 3.220369338989258\n",
      "Epoch 50/100 loss: 3.1193151473999023\n",
      "Epoch 51/100 loss: 3.0257253646850586\n",
      "Epoch 52/100 loss: 2.9351813793182373\n",
      "Epoch 53/100 loss: 2.8508872985839844\n",
      "Epoch 54/100 loss: 2.7702319622039795\n",
      "Epoch 55/100 loss: 2.693875789642334\n",
      "Epoch 56/100 loss: 2.6210670471191406\n",
      "Epoch 57/100 loss: 2.5525059700012207\n",
      "Epoch 58/100 loss: 2.4873313903808594\n",
      "Epoch 59/100 loss: 2.424123764038086\n",
      "Epoch 60/100 loss: 2.3649611473083496\n",
      "Epoch 61/100 loss: 2.308189868927002\n",
      "Epoch 62/100 loss: 2.252960205078125\n",
      "Epoch 63/100 loss: 2.201472043991089\n",
      "Epoch 64/100 loss: 2.15149188041687\n",
      "Epoch 65/100 loss: 2.1038753986358643\n",
      "Epoch 66/100 loss: 2.0577445030212402\n",
      "Epoch 67/100 loss: 2.0139219760894775\n",
      "Epoch 68/100 loss: 1.9715120792388916\n",
      "Epoch 69/100 loss: 1.9306608438491821\n",
      "Epoch 70/100 loss: 1.8917022943496704\n",
      "Epoch 71/100 loss: 1.8540195226669312\n",
      "Epoch 72/100 loss: 1.8176417350769043\n",
      "Epoch 73/100 loss: 1.782729148864746\n",
      "Epoch 74/100 loss: 1.749041199684143\n",
      "Epoch 75/100 loss: 1.7164866924285889\n",
      "Epoch 76/100 loss: 1.6848866939544678\n",
      "Epoch 77/100 loss: 1.654654622077942\n",
      "Epoch 78/100 loss: 1.625177264213562\n",
      "Epoch 79/100 loss: 1.5968139171600342\n",
      "Epoch 80/100 loss: 1.5692745447158813\n",
      "Epoch 81/100 loss: 1.542751669883728\n",
      "Epoch 82/100 loss: 1.5166555643081665\n",
      "Epoch 83/100 loss: 1.491968035697937\n",
      "Epoch 84/100 loss: 1.4675790071487427\n",
      "Epoch 85/100 loss: 1.4439663887023926\n",
      "Epoch 86/100 loss: 1.4213414192199707\n",
      "Epoch 87/100 loss: 1.3990427255630493\n",
      "Epoch 88/100 loss: 1.3775875568389893\n",
      "Epoch 89/100 loss: 1.3565772771835327\n",
      "Epoch 90/100 loss: 1.3363951444625854\n",
      "Epoch 91/100 loss: 1.3166133165359497\n",
      "Epoch 92/100 loss: 1.297318935394287\n",
      "Epoch 93/100 loss: 1.278760552406311\n",
      "Epoch 94/100 loss: 1.260444164276123\n",
      "Epoch 95/100 loss: 1.242575764656067\n",
      "Epoch 96/100 loss: 1.2255820035934448\n",
      "Epoch 97/100 loss: 1.2085613012313843\n",
      "Epoch 98/100 loss: 1.1922640800476074\n",
      "Epoch 99/100 loss: 1.1762609481811523\n"
     ]
    }
   ],
   "source": [
    "epochs =100\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for context_words, target_word in data:\n",
    "        \n",
    "        inputs = make_sentence_vector(context_words, word_dict)\n",
    "        target = [word_dict[target_word]]\n",
    "        model.zero_grad()\n",
    "        log_probs = model(inputs)\n",
    "        loss = loss_function(log_probs, torch.tensor(target, dtype=torch.long))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.data\n",
    "    print(f\"Epoch {epoch}/{epochs} loss: {epoch_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_results(inputs, inverse_word_dict):\n",
    "    index = np.argmax(inputs)\n",
    "    return inverse_word_dict[index]\n",
    "\n",
    "def predict_sentence(sentence):\n",
    "    sentence_split =  sentence.replace(\".\", \"\").lower().split()\n",
    "    sentence_vector =  make_sentence_vector(sentence_split, word_dict)\n",
    "    prediction_array = model(sentence_vector).data.numpy()\n",
    "    predicted_word = get_predicted_results(prediction_array[0], inverse_word_dict)\n",
    "    print(f\"Sentence: {sentence_split[:2]}---- {predicted_word} ----- {sentence_split[2:]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6845, -1.1483,  1.2902,  0.1237,  1.0086,  1.0882,  0.2756, -0.5882,\n",
       "          1.7091, -1.0418,  0.9958,  0.2379,  0.3687,  2.1622,  1.0250, -0.1954,\n",
       "         -0.3950,  0.6367,  0.0842,  1.0751]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_word_embedding(\"leap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if german spacy not present uncomment this:\n",
    "#!python3 -m spacy download de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy  as np\n",
    "from torch import nn, optim\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext.datasets import TranslationDataset, Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import spacy\n",
    "spacy_english = spacy.load('en')\n",
    "spacy_german = spacy.load('de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize languages\n",
    "def tokenize_german(text):\n",
    "    return [token.text for token in spacy_german.tokenizer(text)]\n",
    "\n",
    "# reversing order has been shown to imporve model!!! \n",
    "def tokenize_english(text):\n",
    "    return [token.text for token in spacy_english.tokenizer(text)][::-1] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/FrancescaSogaro/.pyenv/versions/3.8.2/lib/python3.8/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/Users/FrancescaSogaro/.pyenv/versions/3.8.2/lib/python3.8/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "SOURCE = Field(tokenize = tokenize_english, \n",
    "               init_token = \"<sos>\",\n",
    "               eos_token = \"<eso>\",\n",
    "               lower = True\n",
    "              )\n",
    "\n",
    "TARGET = Field(tokenize = tokenize_german, \n",
    "               init_token = \"<sos>\",\n",
    "               eos_token = \"<eso>\",\n",
    "               lower = True\n",
    "              )\n",
    "\n",
    "train_data, valid_data, test_data = Multi30k.splits(exts = ('.en', '.de'),\n",
    "                                                    fields= (SOURCE, TARGET))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', 'bushes', 'many', 'near', 'outside', 'are', 'males', 'white', ',', 'young', 'two']\n",
      "['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.']\n"
     ]
    }
   ],
   "source": [
    "print(train_data.examples[0].src)\n",
    "print(train_data.examples[0].trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29000, 1000, 1000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data.examples), len(test_data.examples), len(test_data.examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5893, 7854)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SOURCE.build_vocab(train_data, min_freq = 2)\n",
    "TARGET.build_vocab(train_data, min_freq = 2)\n",
    "len(SOURCE.vocab), len(TARGET.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device =  torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/FrancescaSogaro/.pyenv/versions/3.8.2/lib/python3.8/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train_data, valid_data, test_data),\n",
    "                                                                      batch_size = batch_size, \n",
    "                                                                      device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dims, emd_dims, hidden_dims, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.hid_dims = hidden_dims\n",
    "        self.emd_dims = emd_dims\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        self.input_dims = input_dims\n",
    "        \n",
    "        self.embeddings = nn.Embedding(input_dims, emd_dims)\n",
    "        self.rnn = nn.LSTM(emd_dims, hidden_dims, n_layers, dropout = dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        embedded = self.dropout(self.embeddings(src))\n",
    "        outputs, (h, cell) = self.rnn(embedded)\n",
    "        return h, cell\n",
    "\n",
    "        \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dims, emd_dims, hidden_dims, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.hid_dims = hidden_dims\n",
    "        self.emd_dims = emd_dims\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        self.output_dims = output_dims\n",
    "        \n",
    "        self.embeddings = nn.Embedding(output_dims, emd_dims)\n",
    "        self.rnn = nn.LSTM(emd_dims, hidden_dims, n_layers, dropout = dropout)\n",
    "        self.fc_out = nn.Linear(hidden_dims, output_dims)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input_data, h, cell):\n",
    "        input_data = input_data.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embeddings(input_data))\n",
    "        outputs, (h, cell) = self.rnn(embedded)\n",
    "        pred = self.fc_out(outputs.squeeze(0))\n",
    "        return pred, h, cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_rate=0.5):\n",
    "        batch_size = trg.shape[1]\n",
    "        target_length = trg.shape[0]\n",
    "        target_vocab_size = self.decoder.output_dims\n",
    "        outputs = torch.zeros(target_length, batch_size, target_vocab_size).to(self.device)\n",
    "        \n",
    "        h, cell = self.encoder(src)\n",
    "        input_tok = trg[0,:]\n",
    "        for t  in range(1, target_length):\n",
    "            output, h, cell = self.decoder(input_tok, h, cell)\n",
    "            outputs[t] = output \n",
    "            top = output.argmax(1)\n",
    "            input_tok = trg[t] if (np.random.random() < teacher_forcing_rate)  else top\n",
    "        return outputs\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an instance ready to be trained:\n",
    "\n",
    "input_dimension = len(SOURCE.vocab)\n",
    "output_dimension = len(TARGET.vocab)\n",
    "encoder_embedding_dimension = 256\n",
    "decoder_embedding_dimension = 256\n",
    "hidden_layer_dimension = 512\n",
    "number_of_layers = 2\n",
    "encoder_dropout = 0.5\n",
    "decoder_dropout = 0.5\n",
    "\n",
    "encod = Encoder(input_dimension, \n",
    "                encoder_embedding_dimension, \n",
    "                hidden_layer_dimension, \n",
    "                number_of_layers, \n",
    "                encoder_dropout)\n",
    "\n",
    "decod = Decoder(output_dimension, \n",
    "                decoder_embedding_dimension, \n",
    "                hidden_layer_dimension, \n",
    "                number_of_layers, \n",
    "                decoder_dropout)\n",
    "\n",
    "model = Seq2Seq(encod, decod, device).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embeddings): Embedding(5893, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embeddings): Embedding(7854, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "    (fc_out): Linear(in_features=512, out_features=7854, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training loop\n",
    "def initialize_weights(m):\n",
    "    for name, param, in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.1, 0.1)\n",
    "\n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TARGET.vocab.stoi[TARGET.pad_token])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg)\n",
    "    \n",
    "        output_dims = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dims)\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss/len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "            # turn off teacher forcing\n",
    "            output = model(src, trg, 0)\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "grad_clip = 1\n",
    "lowest_validation_loss = float('inf')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    training_loss = train(model, train_iterator, optimizer, criterion, grad_clip)\n",
    "    validation_loss = evaluare(model, valid_iterator, optimizer, criterion)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    if validation_loss <  lowest_validation_loss:\n",
    "        lowest_validation_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'seq2seq.pt')\n",
    "    \n",
    "    print(f\"Epoch {epoch}{epochs}, time : {np.round(end_time-start_time)}s\")\n",
    "    print(f\"Train loss: {train_loss.item()}\")\n",
    "    print(f\"VAlid loss: {valid_loss.item()}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

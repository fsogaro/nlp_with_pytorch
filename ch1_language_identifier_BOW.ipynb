{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag Of Words classifier to detect language\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn \n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test data manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [\n",
    "        (\"Veinte paginas\".lower().split(), \"Spanish\"),\n",
    "        (\"I will visit the library\".lower().split(), \"English\"),\n",
    "        (\"I am reading a book\".lower().split(), \"English\"),\n",
    "        (\"This is my favourite chapter\".lower().split(), \"English\"),\n",
    "        (\"Estoy en la biblioteca\".lower().split(), \"Spanish\"),\n",
    "        (\"Tengo un libro\".lower().split(), \"Spanish\")\n",
    "        ]\n",
    "\n",
    "test_data = [\n",
    "        (\"Estoy leyendo\".lower().split(), \"Spanish\"),\n",
    "        (\"This is not my favourite book\".lower().split(), \"English\")\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# generate vocabulary --  I think this is very ineffcient\n",
    "# NB BOW require the use of test set otherwise there would be never seen words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {}\n",
    "i = 0\n",
    "for words, language in training_data + test_data:\n",
    "    for word in words:\n",
    "        if word not in word_dict:\n",
    "            word_dict[word] = i\n",
    "            i +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_size = len(word_dict)\n",
    "languages = 2\n",
    "label_index = {'Spanish': 0, \"English\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the classifier\n",
    "class BowClasssifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, languages, corpus_size):\n",
    "        \n",
    "        super(BowClasssifier, self).__init__()\n",
    "        self.linear = nn.Linear(corpus_size, languages)\n",
    "        \n",
    "    def forward(self, bow_vec):\n",
    "        \n",
    "        l1 = F.log_softmax(self.linear(bow_vec), dim=1)\n",
    "        return l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make util functions\n",
    "def make_bow_vec(sentence, word_dict):\n",
    "    bow_vec = torch.zeros(len(word_dict))\n",
    "    for word in sentence:\n",
    "        bow_vec[word_dict[word]] += 1\n",
    "    return bow_vec.view(1, -1)\n",
    "\n",
    "def make_target(label, label_index):\n",
    "    return torch.LongTensor([label_index[label]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inistantiate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BowClasssifier(languages, corpus_size)\n",
    "loss_function =  nn.NLLLoss()\n",
    "opt = optim.SGD(model.parameters(), lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/100 loss: 0.8330457210540771\n",
      "Epoch 10/100 loss: 0.1341915875673294\n",
      "Epoch 20/100 loss: 0.06795433908700943\n",
      "Epoch 30/100 loss: 0.04517393186688423\n",
      "Epoch 40/100 loss: 0.03376471623778343\n",
      "Epoch 50/100 loss: 0.026934580877423286\n",
      "Epoch 60/100 loss: 0.022393571212887764\n",
      "Epoch 70/100 loss: 0.019158320501446724\n",
      "Epoch 80/100 loss: 0.01673750951886177\n",
      "Epoch 90/100 loss: 0.014858413487672806\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # transform sentence-label pair into bow vector-label\n",
    "    \n",
    "    for sentence, lan  in training_data:\n",
    "        # zero gradient \n",
    "        model.zero_grad()\n",
    "        x_s = make_bow_vec(sentence, word_dict)\n",
    "        y_s = make_target(lan, label_index)\n",
    "        pred = model(x_s)\n",
    "        loss = loss_function(pred, y_s)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    if epoch % 10 ==0:\n",
    "        print(f\"Epoch {epoch}/{epochs} loss: {loss.item()}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estoy leyendo is in 0, and true value is 0\n",
      "this is not my favourite book is in 1, and true value is 1\n"
     ]
    }
   ],
   "source": [
    "# deactivate autograd for evaluation/inference\n",
    "with torch.no_grad():\n",
    "    for sentence, lan  in test_data:\n",
    "        x_s = make_bow_vec(sentence, word_dict)\n",
    "        y_s = make_target(lan, label_index)\n",
    "        pred = model(x_s)\n",
    "        _, y_hat = torch.max(pred, 1)\n",
    "        \n",
    "        print(f\"{' '.join(sentence)} is in {y_hat.item()}, and true value is {y_s.item()}\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'veinte': 0,\n",
       " 'paginas': 1,\n",
       " 'i': 2,\n",
       " 'will': 3,\n",
       " 'visit': 4,\n",
       " 'the': 5,\n",
       " 'library': 6,\n",
       " 'am': 7,\n",
       " 'reading': 8,\n",
       " 'a': 9,\n",
       " 'book': 10,\n",
       " 'this': 11,\n",
       " 'is': 12,\n",
       " 'my': 13,\n",
       " 'favourite': 14,\n",
       " 'chapter': 15,\n",
       " 'estoy': 16,\n",
       " 'en': 17,\n",
       " 'la': 18,\n",
       " 'biblioteca': 19,\n",
       " 'tengo': 20,\n",
       " 'un': 21,\n",
       " 'libro': 22,\n",
       " 'leyendo': 23,\n",
       " 'not': 24}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'some'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-8f60816fa0b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"some\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'some'"
     ]
    }
   ],
   "source": [
    "word_dict[\"some\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_bow_vec([\"not\"], word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model is simple so even with these few data points it knows how to recognise the language\n",
    "# let's interpret the model paramaters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for an imput word, give  me the model paramater\n",
    "\n",
    "def return_param(word):\n",
    "    index = word_dict[word]\n",
    "    for p in model.parameters():\n",
    "        dims = len(p.size())\n",
    "        if dims == 2:\n",
    "            print(f\"{word} : spanish {p[0][index].item()}\" )\n",
    "            print(f\"{word} : english {p[1][index].item()}\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_param(\"not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
